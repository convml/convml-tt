# dataset generation from `convml_tt`

Datasets for `convml_tt` can be generated by describing a "data-source", which
in a unified way allows for the ingestion of data from different sources. As
well as defining the "source" and "type" of your data-source, you can define
the domain and time-span over which to sample data, the number and size of any
triplets you wish to generate and the sampling resolution. These parameters are
all defined through a single `meta.yaml`-file which describes the entire
dataset. An example of a complete configuration-file is given below:

```yaml
source: goes16
type: truecolor_rgb

time:
  t_start: 2020-02-02T14:00
  t_end: 2020-02-02T14:30

domain:
  central_latitude: 13.3 # 13 + (18 / 60)
  central_longitude: -57.72 # -(57 + (43 / 60))
  l_zonal: 200000
  l_meridional: 200000

sampling:
  resolution: 1000.0
  triplets:
    scene_collections_splitting: random_by_relative_sample_size
    N_triplets: {train: 10, study: 2}
    tile_N: 256
```

## The `source` and `type` of your dataset

The first step is to define the `source` and `type`, currently `source in
[goes16, LES]` are supported. 


### `goes16` data source

```yaml
source: goes16
type: truecolor_rgb
```

`source == goes16` will automatically fetch
GOES-16 data from Amazon S3 and with `type==truecolor_rgb` will generate
truecolor RGB composites from the first three radiance channels. has been
implemented.

### `LES` data source

```yaml
source: LES
type: {composite_type}__{field_name1}__{field_name2}...
```

`source == LES` is intended to be used for working with data from Large-Eddy
Simulations or other gridded data in netCDF files. In this case any files
residing in `source_data/LES` be used to generate scenes. The `scene_id` is
generated by using the time-coordinate (assumed to be named `time`) of the
data. `type` in this case simply refers to the name of the variable in the
datasets that will be used (support for multi-channel datasets will be added in
future)

```yaml
source: LES
type: singlechannel__rlut
```

Will create single-channel tiles using the `rlut` field in the provided netCDF
file(s). **NOTE**: for now grey-scale images are created from this single field
which are then fed to the neural network as RGB images. In future it will be
possible to define a userfunction which sets up the channels and input
normalisation for the neural network.


## The timespan of your data-source

In the `time` section of `meta.yaml` you can define the timespan over which you
want to use data. In case of data from the `goes16` source this will be used
both for fetching the correct data and generating scene IDs from this data.
When using a `LES` source this will simply filter scenes from the netCDF files
found. This section can be omitted for `LES` source data, but not for `goes16`
data.

The timespan can be simply defined using `t_start` and `t_end`

```yaml
time:
  t_start: 2020-02-02T14:00
  t_end: 2020-02-02T14:30
```

You can use multiple intervals:

```yaml
time:
  intervals:
    - t_start: 2018-10-01 00:00:00
      t_end: 2019-03-01 00:00:00
    - t_start: 2019-10-01 00:00:00
      t_end: 2020-03-01 00:00:00
    - t_start: 2020-10-01 00:00:00
      t_end: 2021-03-01 00:00:00
  filters:
    N_hours_from_zenith: 4.0
```

And you may optionally list filters to use to only include times match the
filter arguments (at the moment only `N_hours_from_zenith` may be used which
calculates the zenith time in the center of the domain and filters based on
time-difference to this time):

```yaml
time:
  t_start: 2020-02-02T14:00
  t_end: 2020-02-02T14:30
  filters:
    N_hours_from_zenith: 4.0
```


## The sampling `domain` for your data-source

The `domain` section of `meta.yaml` defines the spatial domain within which you
want to use data. If your data is given at lat/lon coordinates it will be
resampled onto a isometric Cartesian grid (by setting `kind == rect`). This is
done by doing a projection onto a tangential plane centered on a point you
define. For `LES` data the `domain` section may be omitted if the whole domain
is to be used (in the case the domain centre will be use for the Cartesian
projection origin).

```yaml
domain:
  central_latitude: 14.0
  central_longitude: -48.0
  l_zonal: 3000.0e+3
  l_meridional: 1000.0e+3
```


## Sampling triplets and the entire domain

To do the tile-based embedding projections using `convml_tt` we must either
feed it individual tiles. There is currently implemented support for producing
triplets of tiles for this purpose or regridding the entire domain at a fixed
resolution and feeding data using a sliding-window to
generate tiles at inference time. In both cases the resolution must be defined
(which is assumed to be given in meters).

```yaml
sampling:
  resolution: 1000.0
  triplets:
    scene_collections_splitting: random_by_relative_sample_size
    N_triplets: {train: 10, study: 2}
    tile_N: 256
```

## Auxiliary data

For some sources (for example `goes16`) there may be auxiliary data fields that
you would like to download and regrid to use during inference time. You should
list the ones you wish to download using the `aux_products` part of the
`meta.yaml` file:

```yaml
aux_products:
  - ACHA
```

# Processing pipeline

To facilitate generating huge datasets and processing all data in parallel the
production of datasets for convml_tt has been implement using the
[luigi](https://luigi.readthedocs.io/) pipeline package by creating individual
luigi `Task`s. Each of these tasks serve a different purpose and the ones you
are most likely to use will be described below. You can also read the source
code yourself in `convml_tt.data.sources.pipeline`.

## Generating scene IDs

```bash
$> python -m luigi --module convml_tt.data.sources.pipeline GenerateSceneIDs
```

## Generate cropped data for all scenes

```bash
luigi --module convml_tt.data.sources.pipeline GenerateCroppedScenes
luigi --module convml_tt.data.sources.pipeline GenerateCroppedScenes --aux-product ACHA
```

# Triplet-based analysis

## Generate tiles

```bash
luigi --module convml_tt.data.sources.pipeline GenerateTiles
```


# Sliding-window based analysis


## Generate sliding window embeddings

```bash
luigi --local-scheduler --module convml_tt.interpretation.rectpred.pipeline.data AggregateFullDatasetImagePredictionMapData --data-path . --step-size 30 --model-path ml-data/fixednorm-stage-2.torch.pkl
```

## Generate regridded data for all scenes

```bash
luigi --module convml_tt.data.sources.pipeline GenerateRegriddedScenes
luigi --module convml_tt.data.sources.pipeline GenerateRegriddedScenes --aux-product ACHA
```

## Plotting optical flow trajectories for all scenes

```bash
$> python -m luigi --module convml_tt.interpretation.rectpred.pipeline.flow PlotAllScenesWithScenePrefixTrajectories
```

## Sliding-window based inference

```bash
luigi --module convml_tt.data.interpretation.rectpred.pipeline AggregateFullDatasetImagePredictionMapData
```

## Transforming embeddings

```bash
luigi --module convml_tt.interpretation.rectpred.pipeline.transforms CreateAllPredictionMapsDataTransformed --embedding-model-path ml-data/fixednorm-stage-2.torch.pkl --step-size 30 --transform-type pca
```

```bash
luigi --module convml_tt.interpretation.rectpred.pipeline.transforms CreateAllPredictionMapsDataTransformed --embedding-model-path ml-data/fixednorm-stage-2.torch.pkl --step-size 30 --pretrained-transform-model pca_transform
```
